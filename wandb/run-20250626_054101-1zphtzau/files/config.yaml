_wandb:
    value:
        cli_version: 0.20.1
        m: []
        python_version: 3.10.12
        t:
            "1":
                - 1
                - 5
                - 11
                - 49
                - 51
                - 53
                - 71
            "2":
                - 1
                - 5
                - 11
                - 41
                - 49
                - 51
                - 53
                - 71
            "3":
                - 16
                - 55
            "4": 3.10.12
            "5": 0.20.1
            "6": 4.52.4
            "12": 0.20.1
            "13": linux-x86_64
BATCH_SIZE:
    value: 2048
CHECKPOINT_DIR:
    value: sae_on_grad/checkpoints_roneneldan_TinyStories-1M_transformer.h.0.attn.attention.v_proj.weight
CONTEXT_LENGTH:
    value: 256
DATASET_NAME:
    value: roneneldan/TinyStories
DEVICE:
    value: cuda
DICT_SIZE:
    value: 8192
DTYPE:
    value: torch.bfloat16
EFFECTIVE_BATCH_SIZE:
    value: 32
GRADIENT_BUFFER_SIZE:
    value: 50000
GRADIENT_SAVE_DIR:
    value: sae_on_grad/gradients_roneneldan_TinyStories-1M_transformer.h.0.attn.attention.v_proj.weight
HF_REPO_ID:
    value: jacobcd52/roneneldan_TinyStories-1M_transformer.h.0.attn.attention.v_proj.weight_sae_k16
LR:
    value: 0.001
MODEL_NAME:
    value: roneneldan/TinyStories-1M
N_EPOCHS:
    value: 1
SAE_K:
    value: 16
TEXT_COLUMN:
    value: text
TOTAL_TRAINING_TOKENS:
    value: 20000000
WANDB_PROJECT:
    value: sae_on_grad
WEIGHT_NAME:
    value: transformer.h.0.attn.attention.v_proj.weight
