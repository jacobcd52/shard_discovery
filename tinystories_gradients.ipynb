{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ All libraries imported successfully\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "import torch\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# HuggingFace libraries\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Import our streamlined modules (token-specific only)\n",
        "from efficient_gradient_collector import (\n",
        "    TokenSpecificGradientCollector, \n",
        "    collect_token_gradients, \n",
        "    load_single_layer_gradients,\n",
        "    analyze_gradient_diversity,\n",
        "    check_existing_gradients,\n",
        "    list_existing_gradients\n",
        ")\n",
        "from visualization_utils import (\n",
        "    create_tsne_with_token_data, \n",
        "    create_umap_with_token_data,\n",
        "    print_token_gradient_summary\n",
        ")\n",
        "\n",
        "print(\"✅ All libraries imported successfully\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🎯 TOKEN-SPECIFIC GRADIENT ANALYSIS\n",
            "   ✅ Each token gets its own gradient (precise attribution)\n",
            "   ✅ Clear current_token → next_token mapping\n",
            "   ✅ Optimized storage format (96x faster loading)\n",
            "   ✅ No reorganization needed (saves directly in efficient format)\n",
            "\n",
            "Configuration set. Saving to: tinystories_gradients\n",
            "Max samples: 100\n",
            "Batch size: 4\n",
            "Save batch size: 100\n",
            "Expected token gradients: ~10,000\n"
          ]
        }
      ],
      "source": [
        "# Configuration for Token-Specific Gradient Analysis\n",
        "MODEL_NAME = \"roneneldan/TinyStories-1M\"\n",
        "DATASET_NAME = \"roneneldan/TinyStories\"\n",
        "SAVE_DIR = \"tinystories_gradients\"\n",
        "\n",
        "print(\"🎯 TOKEN-SPECIFIC GRADIENT ANALYSIS\")\n",
        "print(\"   ✅ Each token gets its own gradient (precise attribution)\")\n",
        "print(\"   ✅ Clear current_token → next_token mapping\")\n",
        "print(\"   ✅ Optimized storage format (96x faster loading)\")\n",
        "print(\"   ✅ No reorganization needed (saves directly in efficient format)\")\n",
        "\n",
        "# Configuration optimized for token-specific gradients\n",
        "MAX_SAMPLES = 100      # Stories to process (each generates ~100 token gradients)\n",
        "MAX_LENGTH = 128       # Maximum sequence length\n",
        "BATCH_SIZE = 4         # Stories processed together\n",
        "SAVE_BATCH_SIZE = 100  # Token gradients saved per batch file\n",
        "TOKEN_CONTEXT_WINDOW = 5  # Context tokens before/after for hover display\n",
        "\n",
        "# Create save directory\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"\\nConfiguration set. Saving to: {SAVE_DIR}\")\n",
        "print(f\"Max samples: {MAX_SAMPLES}\")\n",
        "print(f\"Batch size: {BATCH_SIZE}\")\n",
        "print(f\"Save batch size: {SAVE_BATCH_SIZE}\")\n",
        "print(f\"Expected token gradients: ~{MAX_SAMPLES * 100:,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading model: roneneldan/TinyStories-1M\n",
            "Model loaded on cuda\n",
            "Model type: gpt_neo\n",
            "Tokenizer vocab size: 50257\n",
            "Model parameters: 3,745,984\n",
            "\n",
            "Model architecture:\n",
            "  transformer.wte.weight: torch.Size([50257, 64])\n",
            "  transformer.wpe.weight: torch.Size([2048, 64])\n",
            "  transformer.h.0.ln_1.weight: torch.Size([64])\n",
            "  transformer.h.0.ln_1.bias: torch.Size([64])\n",
            "  transformer.h.0.attn.attention.k_proj.weight: torch.Size([64, 64])\n",
            "  transformer.h.0.attn.attention.v_proj.weight: torch.Size([64, 64])\n",
            "  transformer.h.0.attn.attention.q_proj.weight: torch.Size([64, 64])\n",
            "  transformer.h.0.attn.attention.out_proj.weight: torch.Size([64, 64])\n",
            "  transformer.h.0.attn.attention.out_proj.bias: torch.Size([64])\n",
            "  transformer.h.0.ln_2.weight: torch.Size([64])\n",
            "  transformer.h.0.ln_2.bias: torch.Size([64])\n",
            "  transformer.h.0.mlp.c_fc.weight: torch.Size([256, 64])\n",
            "  transformer.h.0.mlp.c_fc.bias: torch.Size([256])\n",
            "  transformer.h.0.mlp.c_proj.weight: torch.Size([64, 256])\n",
            "  transformer.h.0.mlp.c_proj.bias: torch.Size([64])\n",
            "  transformer.h.1.ln_1.weight: torch.Size([64])\n",
            "  transformer.h.1.ln_1.bias: torch.Size([64])\n",
            "  transformer.h.1.attn.attention.k_proj.weight: torch.Size([64, 64])\n",
            "  transformer.h.1.attn.attention.v_proj.weight: torch.Size([64, 64])\n",
            "  transformer.h.1.attn.attention.q_proj.weight: torch.Size([64, 64])\n",
            "  transformer.h.1.attn.attention.out_proj.weight: torch.Size([64, 64])\n",
            "  transformer.h.1.attn.attention.out_proj.bias: torch.Size([64])\n",
            "  transformer.h.1.ln_2.weight: torch.Size([64])\n",
            "  transformer.h.1.ln_2.bias: torch.Size([64])\n",
            "  transformer.h.1.mlp.c_fc.weight: torch.Size([256, 64])\n",
            "  transformer.h.1.mlp.c_fc.bias: torch.Size([256])\n",
            "  transformer.h.1.mlp.c_proj.weight: torch.Size([64, 256])\n",
            "  transformer.h.1.mlp.c_proj.bias: torch.Size([64])\n",
            "  transformer.h.2.ln_1.weight: torch.Size([64])\n",
            "  transformer.h.2.ln_1.bias: torch.Size([64])\n",
            "  transformer.h.2.attn.attention.k_proj.weight: torch.Size([64, 64])\n",
            "  transformer.h.2.attn.attention.v_proj.weight: torch.Size([64, 64])\n",
            "  transformer.h.2.attn.attention.q_proj.weight: torch.Size([64, 64])\n",
            "  transformer.h.2.attn.attention.out_proj.weight: torch.Size([64, 64])\n",
            "  transformer.h.2.attn.attention.out_proj.bias: torch.Size([64])\n",
            "  transformer.h.2.ln_2.weight: torch.Size([64])\n",
            "  transformer.h.2.ln_2.bias: torch.Size([64])\n",
            "  transformer.h.2.mlp.c_fc.weight: torch.Size([256, 64])\n",
            "  transformer.h.2.mlp.c_fc.bias: torch.Size([256])\n",
            "  transformer.h.2.mlp.c_proj.weight: torch.Size([64, 256])\n",
            "  transformer.h.2.mlp.c_proj.bias: torch.Size([64])\n",
            "  transformer.h.3.ln_1.weight: torch.Size([64])\n",
            "  transformer.h.3.ln_1.bias: torch.Size([64])\n",
            "  transformer.h.3.attn.attention.k_proj.weight: torch.Size([64, 64])\n",
            "  transformer.h.3.attn.attention.v_proj.weight: torch.Size([64, 64])\n",
            "  transformer.h.3.attn.attention.q_proj.weight: torch.Size([64, 64])\n",
            "  transformer.h.3.attn.attention.out_proj.weight: torch.Size([64, 64])\n",
            "  transformer.h.3.attn.attention.out_proj.bias: torch.Size([64])\n",
            "  transformer.h.3.ln_2.weight: torch.Size([64])\n",
            "  transformer.h.3.ln_2.bias: torch.Size([64])\n",
            "  transformer.h.3.mlp.c_fc.weight: torch.Size([256, 64])\n",
            "  transformer.h.3.mlp.c_fc.bias: torch.Size([256])\n",
            "  transformer.h.3.mlp.c_proj.weight: torch.Size([64, 256])\n",
            "  transformer.h.3.mlp.c_proj.bias: torch.Size([64])\n",
            "  transformer.h.4.ln_1.weight: torch.Size([64])\n",
            "  transformer.h.4.ln_1.bias: torch.Size([64])\n",
            "  transformer.h.4.attn.attention.k_proj.weight: torch.Size([64, 64])\n",
            "  transformer.h.4.attn.attention.v_proj.weight: torch.Size([64, 64])\n",
            "  transformer.h.4.attn.attention.q_proj.weight: torch.Size([64, 64])\n",
            "  transformer.h.4.attn.attention.out_proj.weight: torch.Size([64, 64])\n",
            "  transformer.h.4.attn.attention.out_proj.bias: torch.Size([64])\n",
            "  transformer.h.4.ln_2.weight: torch.Size([64])\n",
            "  transformer.h.4.ln_2.bias: torch.Size([64])\n",
            "  transformer.h.4.mlp.c_fc.weight: torch.Size([256, 64])\n",
            "  transformer.h.4.mlp.c_fc.bias: torch.Size([256])\n",
            "  transformer.h.4.mlp.c_proj.weight: torch.Size([64, 256])\n",
            "  transformer.h.4.mlp.c_proj.bias: torch.Size([64])\n",
            "  transformer.h.5.ln_1.weight: torch.Size([64])\n",
            "  transformer.h.5.ln_1.bias: torch.Size([64])\n",
            "  transformer.h.5.attn.attention.k_proj.weight: torch.Size([64, 64])\n",
            "  transformer.h.5.attn.attention.v_proj.weight: torch.Size([64, 64])\n",
            "  transformer.h.5.attn.attention.q_proj.weight: torch.Size([64, 64])\n",
            "  transformer.h.5.attn.attention.out_proj.weight: torch.Size([64, 64])\n",
            "  transformer.h.5.attn.attention.out_proj.bias: torch.Size([64])\n",
            "  transformer.h.5.ln_2.weight: torch.Size([64])\n",
            "  transformer.h.5.ln_2.bias: torch.Size([64])\n",
            "  transformer.h.5.mlp.c_fc.weight: torch.Size([256, 64])\n",
            "  transformer.h.5.mlp.c_fc.bias: torch.Size([256])\n",
            "  transformer.h.5.mlp.c_proj.weight: torch.Size([64, 256])\n",
            "  transformer.h.5.mlp.c_proj.bias: torch.Size([64])\n",
            "  transformer.h.6.ln_1.weight: torch.Size([64])\n",
            "  transformer.h.6.ln_1.bias: torch.Size([64])\n",
            "  transformer.h.6.attn.attention.k_proj.weight: torch.Size([64, 64])\n",
            "  transformer.h.6.attn.attention.v_proj.weight: torch.Size([64, 64])\n",
            "  transformer.h.6.attn.attention.q_proj.weight: torch.Size([64, 64])\n",
            "  transformer.h.6.attn.attention.out_proj.weight: torch.Size([64, 64])\n",
            "  transformer.h.6.attn.attention.out_proj.bias: torch.Size([64])\n",
            "  transformer.h.6.ln_2.weight: torch.Size([64])\n",
            "  transformer.h.6.ln_2.bias: torch.Size([64])\n",
            "  transformer.h.6.mlp.c_fc.weight: torch.Size([256, 64])\n",
            "  transformer.h.6.mlp.c_fc.bias: torch.Size([256])\n",
            "  transformer.h.6.mlp.c_proj.weight: torch.Size([64, 256])\n",
            "  transformer.h.6.mlp.c_proj.bias: torch.Size([64])\n",
            "  transformer.h.7.ln_1.weight: torch.Size([64])\n",
            "  transformer.h.7.ln_1.bias: torch.Size([64])\n",
            "  transformer.h.7.attn.attention.k_proj.weight: torch.Size([64, 64])\n",
            "  transformer.h.7.attn.attention.v_proj.weight: torch.Size([64, 64])\n",
            "  transformer.h.7.attn.attention.q_proj.weight: torch.Size([64, 64])\n",
            "  transformer.h.7.attn.attention.out_proj.weight: torch.Size([64, 64])\n",
            "  transformer.h.7.attn.attention.out_proj.bias: torch.Size([64])\n",
            "  transformer.h.7.ln_2.weight: torch.Size([64])\n",
            "  transformer.h.7.ln_2.bias: torch.Size([64])\n",
            "  transformer.h.7.mlp.c_fc.weight: torch.Size([256, 64])\n",
            "  transformer.h.7.mlp.c_fc.bias: torch.Size([256])\n",
            "  transformer.h.7.mlp.c_proj.weight: torch.Size([64, 256])\n",
            "  transformer.h.7.mlp.c_proj.bias: torch.Size([64])\n",
            "  transformer.ln_f.weight: torch.Size([64])\n",
            "  transformer.ln_f.bias: torch.Size([64])\n"
          ]
        }
      ],
      "source": [
        "# Load TinyStories model and tokenizer\n",
        "print(f\"Loading model: {MODEL_NAME}\")\n",
        "\n",
        "# Load tokenizer (use GPT-Neo tokenizer as specified in model card)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-125M\")\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Load model\n",
        "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "\n",
        "print(f\"Model loaded on {device}\")\n",
        "print(f\"Model type: {model.config.model_type}\")\n",
        "print(f\"Tokenizer vocab size: {tokenizer.vocab_size}\")\n",
        "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "\n",
        "# Show model architecture\n",
        "print(\"\\nModel architecture:\")\n",
        "for name, param in model.named_parameters():\n",
        "    if param.requires_grad:\n",
        "        print(f\"  {name}: {param.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading dataset: roneneldan/TinyStories\n",
            "Loaded 100 stories\n",
            "\n",
            "Example story:\n",
            "'One day, a little girl named Lily found a needle in her room. She knew it was difficult to play with it because it was sharp. Lily wanted to share the needle with her mom, so she could sew a button on...'\n"
          ]
        }
      ],
      "source": [
        "# Load TinyStories dataset\n",
        "print(f\"Loading dataset: {DATASET_NAME}\")\n",
        "\n",
        "dataset = load_dataset(DATASET_NAME, split=\"train\", streaming=True)\n",
        "\n",
        "# Sample some stories\n",
        "stories = []\n",
        "for i, example in enumerate(dataset):\n",
        "    if i >= MAX_SAMPLES:\n",
        "        break\n",
        "    stories.append(example['text'])\n",
        "\n",
        "print(f\"Loaded {len(stories)} stories\")\n",
        "print(f\"\\nExample story:\")\n",
        "print(f\"'{stories[0][:200]}...'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🎯 Starting token-specific gradient collection...\n",
            "   Each token prediction gets its own gradient\n",
            "   Saving directly in optimized per-layer format\n",
            "🎯 Token-specific gradient collection initialized\n",
            "Tracking 108 parameters\n",
            "Will save batches of 100 token gradients\n",
            "Using optimized per-layer storage format\n",
            "Using device: cuda:0 (GPU: True)\n",
            "🎯 Starting token-specific gradient collection...\n",
            "   Processing 100 stories\n",
            "   Each token gets its own gradient (precise attribution!)\n",
            "   Saving directly in optimized format (no reorganization needed)\n",
            "Processed 50 token positions (saved 0 batches)\n",
            "💾 Saved optimized batch 0 with 100 token gradients\n",
            "Processed 100 token positions (saved 1 batches)\n",
            "Processed 150 token positions (saved 1 batches)\n",
            "💾 Saved optimized batch 1 with 100 token gradients\n",
            "Processed 200 token positions (saved 2 batches)\n",
            "Processed 250 token positions (saved 2 batches)\n",
            "💾 Saved optimized batch 2 with 100 token gradients\n",
            "Processed 300 token positions (saved 3 batches)\n",
            "Processed 350 token positions (saved 3 batches)\n",
            "💾 Saved optimized batch 3 with 100 token gradients\n",
            "Processed 400 token positions (saved 4 batches)\n",
            "Processed 450 token positions (saved 4 batches)\n",
            "💾 Saved optimized batch 4 with 100 token gradients\n",
            "Processed 500 token positions (saved 5 batches)\n",
            "Processed 4 / 100 stories\n",
            "Processed 550 token positions (saved 5 batches)\n",
            "💾 Saved optimized batch 5 with 100 token gradients\n",
            "Processed 600 token positions (saved 6 batches)\n",
            "Processed 650 token positions (saved 6 batches)\n",
            "💾 Saved optimized batch 6 with 100 token gradients\n",
            "Processed 700 token positions (saved 7 batches)\n",
            "Processed 750 token positions (saved 7 batches)\n",
            "💾 Saved optimized batch 7 with 100 token gradients\n",
            "Processed 800 token positions (saved 8 batches)\n",
            "Processed 850 token positions (saved 8 batches)\n",
            "💾 Saved optimized batch 8 with 100 token gradients\n",
            "Processed 900 token positions (saved 9 batches)\n",
            "Processed 950 token positions (saved 9 batches)\n",
            "💾 Saved optimized batch 9 with 100 token gradients\n",
            "Processed 1000 token positions (saved 10 batches)\n",
            "Processed 1050 token positions (saved 10 batches)\n",
            "💾 Saved optimized batch 10 with 100 token gradients\n",
            "Processed 1100 token positions (saved 11 batches)\n",
            "Processed 1150 token positions (saved 11 batches)\n",
            "💾 Saved optimized batch 11 with 100 token gradients\n",
            "Processed 1200 token positions (saved 12 batches)\n",
            "Processed 1250 token positions (saved 12 batches)\n",
            "💾 Saved optimized batch 12 with 100 token gradients\n",
            "Processed 1300 token positions (saved 13 batches)\n",
            "Processed 1350 token positions (saved 13 batches)\n",
            "💾 Saved optimized batch 13 with 100 token gradients\n",
            "Processed 1400 token positions (saved 14 batches)\n",
            "Processed 1450 token positions (saved 14 batches)\n",
            "💾 Saved optimized batch 14 with 100 token gradients\n",
            "Processed 1500 token positions (saved 15 batches)\n",
            "Processed 1550 token positions (saved 15 batches)\n",
            "💾 Saved optimized batch 15 with 100 token gradients\n",
            "Processed 1600 token positions (saved 16 batches)\n",
            "Processed 1650 token positions (saved 16 batches)\n",
            "💾 Saved optimized batch 16 with 100 token gradients\n",
            "Processed 1700 token positions (saved 17 batches)\n",
            "Processed 1750 token positions (saved 17 batches)\n",
            "💾 Saved optimized batch 17 with 100 token gradients\n",
            "Processed 1800 token positions (saved 18 batches)\n",
            "Processed 1850 token positions (saved 18 batches)\n",
            "💾 Saved optimized batch 18 with 100 token gradients\n",
            "Processed 1900 token positions (saved 19 batches)\n",
            "Processed 1950 token positions (saved 19 batches)\n",
            "💾 Saved optimized batch 19 with 100 token gradients\n",
            "Processed 2000 token positions (saved 20 batches)\n",
            "Processed 2050 token positions (saved 20 batches)\n",
            "💾 Saved optimized batch 20 with 100 token gradients\n",
            "Processed 2100 token positions (saved 21 batches)\n",
            "Processed 2150 token positions (saved 21 batches)\n",
            "💾 Saved optimized batch 21 with 100 token gradients\n",
            "Processed 2200 token positions (saved 22 batches)\n",
            "Processed 2250 token positions (saved 22 batches)\n",
            "💾 Saved optimized batch 22 with 100 token gradients\n",
            "Processed 2300 token positions (saved 23 batches)\n",
            "Processed 2350 token positions (saved 23 batches)\n",
            "💾 Saved optimized batch 23 with 100 token gradients\n",
            "Processed 2400 token positions (saved 24 batches)\n",
            "Processed 2450 token positions (saved 24 batches)\n",
            "💾 Saved optimized batch 24 with 100 token gradients\n",
            "Processed 2500 token positions (saved 25 batches)\n",
            "Processed 2550 token positions (saved 25 batches)\n",
            "💾 Saved optimized batch 25 with 100 token gradients\n",
            "Processed 2600 token positions (saved 26 batches)\n",
            "Processed 2650 token positions (saved 26 batches)\n",
            "💾 Saved optimized batch 26 with 100 token gradients\n",
            "Processed 2700 token positions (saved 27 batches)\n",
            "Processed 2750 token positions (saved 27 batches)\n",
            "💾 Saved optimized batch 27 with 100 token gradients\n",
            "Processed 2800 token positions (saved 28 batches)\n",
            "Processed 2850 token positions (saved 28 batches)\n",
            "💾 Saved optimized batch 28 with 100 token gradients\n",
            "Processed 2900 token positions (saved 29 batches)\n",
            "Processed 2950 token positions (saved 29 batches)\n",
            "Processed 24 / 100 stories\n",
            "💾 Saved optimized batch 29 with 100 token gradients\n",
            "Processed 3000 token positions (saved 30 batches)\n",
            "Processed 3050 token positions (saved 30 batches)\n",
            "💾 Saved optimized batch 30 with 100 token gradients\n",
            "Processed 3100 token positions (saved 31 batches)\n",
            "Processed 3150 token positions (saved 31 batches)\n",
            "💾 Saved optimized batch 31 with 100 token gradients\n",
            "Processed 3200 token positions (saved 32 batches)\n",
            "Processed 3250 token positions (saved 32 batches)\n",
            "💾 Saved optimized batch 32 with 100 token gradients\n",
            "Processed 3300 token positions (saved 33 batches)\n",
            "Processed 3350 token positions (saved 33 batches)\n",
            "💾 Saved optimized batch 33 with 100 token gradients\n",
            "Processed 3400 token positions (saved 34 batches)\n",
            "Processed 3450 token positions (saved 34 batches)\n",
            "💾 Saved optimized batch 34 with 100 token gradients\n",
            "Processed 3500 token positions (saved 35 batches)\n",
            "Processed 3550 token positions (saved 35 batches)\n",
            "💾 Saved optimized batch 35 with 100 token gradients\n",
            "Processed 3600 token positions (saved 36 batches)\n",
            "Processed 3650 token positions (saved 36 batches)\n",
            "💾 Saved optimized batch 36 with 100 token gradients\n",
            "Processed 3700 token positions (saved 37 batches)\n",
            "Processed 3750 token positions (saved 37 batches)\n",
            "💾 Saved optimized batch 37 with 100 token gradients\n",
            "Processed 3800 token positions (saved 38 batches)\n",
            "Processed 3850 token positions (saved 38 batches)\n",
            "💾 Saved optimized batch 38 with 100 token gradients\n",
            "Processed 3900 token positions (saved 39 batches)\n",
            "Processed 3950 token positions (saved 39 batches)\n",
            "💾 Saved optimized batch 39 with 100 token gradients\n",
            "Processed 4000 token positions (saved 40 batches)\n",
            "Processed 4050 token positions (saved 40 batches)\n",
            "💾 Saved optimized batch 40 with 100 token gradients\n",
            "Processed 4100 token positions (saved 41 batches)\n",
            "Processed 4150 token positions (saved 41 batches)\n",
            "💾 Saved optimized batch 41 with 100 token gradients\n",
            "Processed 4200 token positions (saved 42 batches)\n",
            "Processed 4250 token positions (saved 42 batches)\n",
            "💾 Saved optimized batch 42 with 100 token gradients\n",
            "Processed 4300 token positions (saved 43 batches)\n",
            "Processed 4350 token positions (saved 43 batches)\n",
            "💾 Saved optimized batch 43 with 100 token gradients\n",
            "Processed 4400 token positions (saved 44 batches)\n",
            "Processed 4450 token positions (saved 44 batches)\n",
            "💾 Saved optimized batch 44 with 100 token gradients\n",
            "Processed 4500 token positions (saved 45 batches)\n",
            "Processed 4550 token positions (saved 45 batches)\n",
            "💾 Saved optimized batch 45 with 100 token gradients\n",
            "Processed 4600 token positions (saved 46 batches)\n",
            "Processed 4650 token positions (saved 46 batches)\n",
            "💾 Saved optimized batch 46 with 100 token gradients\n",
            "Processed 4700 token positions (saved 47 batches)\n",
            "Processed 4750 token positions (saved 47 batches)\n",
            "💾 Saved optimized batch 47 with 100 token gradients\n",
            "Processed 4800 token positions (saved 48 batches)\n",
            "Processed 4850 token positions (saved 48 batches)\n",
            "💾 Saved optimized batch 48 with 100 token gradients\n",
            "Processed 4900 token positions (saved 49 batches)\n",
            "Processed 4950 token positions (saved 49 batches)\n",
            "💾 Saved optimized batch 49 with 100 token gradients\n",
            "Processed 5000 token positions (saved 50 batches)\n",
            "Processed 5050 token positions (saved 50 batches)\n",
            "💾 Saved optimized batch 50 with 100 token gradients\n",
            "Processed 5100 token positions (saved 51 batches)\n",
            "Processed 5150 token positions (saved 51 batches)\n",
            "💾 Saved optimized batch 51 with 100 token gradients\n",
            "Processed 5200 token positions (saved 52 batches)\n",
            "Processed 5250 token positions (saved 52 batches)\n",
            "💾 Saved optimized batch 52 with 100 token gradients\n",
            "Processed 5300 token positions (saved 53 batches)\n",
            "Processed 5350 token positions (saved 53 batches)\n",
            "💾 Saved optimized batch 53 with 100 token gradients\n",
            "Processed 5400 token positions (saved 54 batches)\n",
            "Processed 5450 token positions (saved 54 batches)\n",
            "💾 Saved optimized batch 54 with 100 token gradients\n",
            "Processed 5500 token positions (saved 55 batches)\n",
            "Processed 44 / 100 stories\n",
            "Processed 5550 token positions (saved 55 batches)\n",
            "💾 Saved optimized batch 55 with 100 token gradients\n",
            "Processed 5600 token positions (saved 56 batches)\n",
            "Processed 5650 token positions (saved 56 batches)\n",
            "💾 Saved optimized batch 56 with 100 token gradients\n",
            "Processed 5700 token positions (saved 57 batches)\n",
            "Processed 5750 token positions (saved 57 batches)\n",
            "💾 Saved optimized batch 57 with 100 token gradients\n",
            "Processed 5800 token positions (saved 58 batches)\n",
            "Processed 5850 token positions (saved 58 batches)\n",
            "💾 Saved optimized batch 58 with 100 token gradients\n",
            "Processed 5900 token positions (saved 59 batches)\n",
            "Processed 5950 token positions (saved 59 batches)\n",
            "💾 Saved optimized batch 59 with 100 token gradients\n",
            "Processed 6000 token positions (saved 60 batches)\n",
            "Processed 6050 token positions (saved 60 batches)\n",
            "💾 Saved optimized batch 60 with 100 token gradients\n",
            "Processed 6100 token positions (saved 61 batches)\n",
            "Processed 6150 token positions (saved 61 batches)\n",
            "💾 Saved optimized batch 61 with 100 token gradients\n",
            "Processed 6200 token positions (saved 62 batches)\n",
            "Processed 6250 token positions (saved 62 batches)\n",
            "💾 Saved optimized batch 62 with 100 token gradients\n",
            "Processed 6300 token positions (saved 63 batches)\n",
            "Processed 6350 token positions (saved 63 batches)\n",
            "💾 Saved optimized batch 63 with 100 token gradients\n",
            "Processed 6400 token positions (saved 64 batches)\n",
            "Processed 6450 token positions (saved 64 batches)\n",
            "💾 Saved optimized batch 64 with 100 token gradients\n",
            "Processed 6500 token positions (saved 65 batches)\n",
            "Processed 6550 token positions (saved 65 batches)\n",
            "💾 Saved optimized batch 65 with 100 token gradients\n",
            "Processed 6600 token positions (saved 66 batches)\n",
            "Processed 6650 token positions (saved 66 batches)\n",
            "💾 Saved optimized batch 66 with 100 token gradients\n",
            "Processed 6700 token positions (saved 67 batches)\n",
            "Processed 6750 token positions (saved 67 batches)\n",
            "💾 Saved optimized batch 67 with 100 token gradients\n",
            "Processed 6800 token positions (saved 68 batches)\n",
            "Processed 6850 token positions (saved 68 batches)\n",
            "💾 Saved optimized batch 68 with 100 token gradients\n",
            "Processed 6900 token positions (saved 69 batches)\n",
            "Processed 6950 token positions (saved 69 batches)\n",
            "💾 Saved optimized batch 69 with 100 token gradients\n",
            "Processed 7000 token positions (saved 70 batches)\n",
            "Processed 7050 token positions (saved 70 batches)\n",
            "💾 Saved optimized batch 70 with 100 token gradients\n",
            "Processed 7100 token positions (saved 71 batches)\n",
            "Processed 7150 token positions (saved 71 batches)\n",
            "💾 Saved optimized batch 71 with 100 token gradients\n",
            "Processed 7200 token positions (saved 72 batches)\n",
            "Processed 7250 token positions (saved 72 batches)\n",
            "💾 Saved optimized batch 72 with 100 token gradients\n",
            "Processed 7300 token positions (saved 73 batches)\n",
            "Processed 7350 token positions (saved 73 batches)\n",
            "💾 Saved optimized batch 73 with 100 token gradients\n",
            "Processed 7400 token positions (saved 74 batches)\n",
            "Processed 7450 token positions (saved 74 batches)\n",
            "💾 Saved optimized batch 74 with 100 token gradients\n",
            "Processed 7500 token positions (saved 75 batches)\n",
            "Processed 7550 token positions (saved 75 batches)\n",
            "💾 Saved optimized batch 75 with 100 token gradients\n",
            "Processed 7600 token positions (saved 76 batches)\n",
            "Processed 7650 token positions (saved 76 batches)\n",
            "💾 Saved optimized batch 76 with 100 token gradients\n",
            "Processed 7700 token positions (saved 77 batches)\n",
            "Processed 7750 token positions (saved 77 batches)\n",
            "💾 Saved optimized batch 77 with 100 token gradients\n",
            "Processed 7800 token positions (saved 78 batches)\n",
            "Processed 7850 token positions (saved 78 batches)\n",
            "💾 Saved optimized batch 78 with 100 token gradients\n",
            "Processed 7900 token positions (saved 79 batches)\n",
            "Processed 7950 token positions (saved 79 batches)\n",
            "💾 Saved optimized batch 79 with 100 token gradients\n",
            "Processed 8000 token positions (saved 80 batches)\n",
            "Processed 8050 token positions (saved 80 batches)\n",
            "Processed 64 / 100 stories\n",
            "💾 Saved optimized batch 80 with 100 token gradients\n",
            "Processed 8100 token positions (saved 81 batches)\n",
            "Processed 8150 token positions (saved 81 batches)\n",
            "💾 Saved optimized batch 81 with 100 token gradients\n",
            "Processed 8200 token positions (saved 82 batches)\n",
            "Processed 8250 token positions (saved 82 batches)\n",
            "💾 Saved optimized batch 82 with 100 token gradients\n",
            "Processed 8300 token positions (saved 83 batches)\n",
            "Processed 8350 token positions (saved 83 batches)\n",
            "💾 Saved optimized batch 83 with 100 token gradients\n",
            "Processed 8400 token positions (saved 84 batches)\n",
            "Processed 8450 token positions (saved 84 batches)\n",
            "💾 Saved optimized batch 84 with 100 token gradients\n",
            "Processed 8500 token positions (saved 85 batches)\n",
            "Processed 8550 token positions (saved 85 batches)\n",
            "💾 Saved optimized batch 85 with 100 token gradients\n",
            "Processed 8600 token positions (saved 86 batches)\n",
            "Processed 8650 token positions (saved 86 batches)\n",
            "💾 Saved optimized batch 86 with 100 token gradients\n",
            "Processed 8700 token positions (saved 87 batches)\n",
            "Processed 8750 token positions (saved 87 batches)\n",
            "💾 Saved optimized batch 87 with 100 token gradients\n",
            "Processed 8800 token positions (saved 88 batches)\n",
            "Processed 8850 token positions (saved 88 batches)\n",
            "💾 Saved optimized batch 88 with 100 token gradients\n",
            "Processed 8900 token positions (saved 89 batches)\n",
            "Processed 8950 token positions (saved 89 batches)\n",
            "💾 Saved optimized batch 89 with 100 token gradients\n",
            "Processed 9000 token positions (saved 90 batches)\n",
            "Processed 9050 token positions (saved 90 batches)\n",
            "💾 Saved optimized batch 90 with 100 token gradients\n",
            "Processed 9100 token positions (saved 91 batches)\n",
            "Processed 9150 token positions (saved 91 batches)\n",
            "💾 Saved optimized batch 91 with 100 token gradients\n",
            "Processed 9200 token positions (saved 92 batches)\n",
            "Processed 9250 token positions (saved 92 batches)\n",
            "💾 Saved optimized batch 92 with 100 token gradients\n",
            "Processed 9300 token positions (saved 93 batches)\n",
            "Processed 9350 token positions (saved 93 batches)\n",
            "💾 Saved optimized batch 93 with 100 token gradients\n",
            "Processed 9400 token positions (saved 94 batches)\n",
            "Processed 9450 token positions (saved 94 batches)\n",
            "💾 Saved optimized batch 94 with 100 token gradients\n",
            "Processed 9500 token positions (saved 95 batches)\n",
            "Processed 9550 token positions (saved 95 batches)\n",
            "💾 Saved optimized batch 95 with 100 token gradients\n",
            "Processed 9600 token positions (saved 96 batches)\n",
            "Processed 9650 token positions (saved 96 batches)\n",
            "💾 Saved optimized batch 96 with 100 token gradients\n",
            "Processed 9700 token positions (saved 97 batches)\n",
            "Processed 9750 token positions (saved 97 batches)\n",
            "💾 Saved optimized batch 97 with 100 token gradients\n",
            "Processed 9800 token positions (saved 98 batches)\n",
            "Processed 9850 token positions (saved 98 batches)\n",
            "💾 Saved optimized batch 98 with 100 token gradients\n",
            "Processed 9900 token positions (saved 99 batches)\n",
            "Processed 9950 token positions (saved 99 batches)\n",
            "💾 Saved optimized batch 99 with 100 token gradients\n",
            "Processed 10000 token positions (saved 100 batches)\n",
            "Processed 10050 token positions (saved 100 batches)\n",
            "💾 Saved optimized batch 100 with 100 token gradients\n",
            "Processed 10100 token positions (saved 101 batches)\n",
            "Processed 10150 token positions (saved 101 batches)\n",
            "💾 Saved optimized batch 101 with 100 token gradients\n",
            "Processed 10200 token positions (saved 102 batches)\n",
            "Processed 10250 token positions (saved 102 batches)\n",
            "💾 Saved optimized batch 102 with 100 token gradients\n",
            "Processed 10300 token positions (saved 103 batches)\n",
            "Processed 10350 token positions (saved 103 batches)\n",
            "💾 Saved optimized batch 103 with 100 token gradients\n",
            "Processed 10400 token positions (saved 104 batches)\n",
            "Processed 10450 token positions (saved 104 batches)\n",
            "💾 Saved optimized batch 104 with 100 token gradients\n",
            "Processed 10500 token positions (saved 105 batches)\n",
            "Processed 10550 token positions (saved 105 batches)\n",
            "Processed 84 / 100 stories\n",
            "💾 Saved optimized batch 105 with 100 token gradients\n",
            "Processed 10600 token positions (saved 106 batches)\n",
            "Processed 10650 token positions (saved 106 batches)\n",
            "💾 Saved optimized batch 106 with 100 token gradients\n",
            "Processed 10700 token positions (saved 107 batches)\n",
            "Processed 10750 token positions (saved 107 batches)\n",
            "💾 Saved optimized batch 107 with 100 token gradients\n",
            "Processed 10800 token positions (saved 108 batches)\n",
            "Processed 10850 token positions (saved 108 batches)\n",
            "💾 Saved optimized batch 108 with 100 token gradients\n",
            "Processed 10900 token positions (saved 109 batches)\n",
            "Processed 10950 token positions (saved 109 batches)\n",
            "💾 Saved optimized batch 109 with 100 token gradients\n",
            "Processed 11000 token positions (saved 110 batches)\n",
            "Processed 11050 token positions (saved 110 batches)\n",
            "💾 Saved optimized batch 110 with 100 token gradients\n",
            "Processed 11100 token positions (saved 111 batches)\n",
            "Processed 11150 token positions (saved 111 batches)\n",
            "💾 Saved optimized batch 111 with 100 token gradients\n",
            "Processed 11200 token positions (saved 112 batches)\n",
            "Processed 11250 token positions (saved 112 batches)\n",
            "💾 Saved optimized batch 112 with 100 token gradients\n",
            "Processed 11300 token positions (saved 113 batches)\n",
            "Processed 11350 token positions (saved 113 batches)\n",
            "💾 Saved optimized batch 113 with 100 token gradients\n",
            "Processed 11400 token positions (saved 114 batches)\n",
            "Processed 11450 token positions (saved 114 batches)\n",
            "💾 Saved optimized batch 114 with 100 token gradients\n",
            "Processed 11500 token positions (saved 115 batches)\n",
            "Processed 11550 token positions (saved 115 batches)\n",
            "💾 Saved optimized batch 115 with 100 token gradients\n",
            "Processed 11600 token positions (saved 116 batches)\n",
            "Processed 11650 token positions (saved 116 batches)\n",
            "💾 Saved optimized batch 116 with 100 token gradients\n",
            "Processed 11700 token positions (saved 117 batches)\n",
            "Processed 11750 token positions (saved 117 batches)\n",
            "💾 Saved optimized batch 117 with 100 token gradients\n",
            "Processed 11800 token positions (saved 118 batches)\n",
            "Processed 11850 token positions (saved 118 batches)\n",
            "💾 Saved optimized batch 118 with 100 token gradients\n",
            "Processed 11900 token positions (saved 119 batches)\n",
            "Processed 11950 token positions (saved 119 batches)\n",
            "💾 Saved optimized batch 119 with 100 token gradients\n",
            "Processed 12000 token positions (saved 120 batches)\n",
            "Processed 12050 token positions (saved 120 batches)\n",
            "💾 Saved optimized batch 120 with 100 token gradients\n",
            "Processed 12100 token positions (saved 121 batches)\n",
            "Processed 12150 token positions (saved 121 batches)\n",
            "💾 Saved optimized batch 121 with 100 token gradients\n",
            "Processed 12200 token positions (saved 122 batches)\n",
            "Processed 12250 token positions (saved 122 batches)\n",
            "💾 Saved optimized batch 122 with 100 token gradients\n",
            "Processed 12300 token positions (saved 123 batches)\n",
            "Processed 12350 token positions (saved 123 batches)\n",
            "💾 Saved optimized batch 123 with 100 token gradients\n",
            "Processed 12400 token positions (saved 124 batches)\n",
            "Processed 12450 token positions (saved 124 batches)\n",
            "💾 Saved optimized batch 124 with 100 token gradients\n",
            "Processed 12500 token positions (saved 125 batches)\n",
            "Processed 12550 token positions (saved 125 batches)\n",
            "💾 Saved optimized batch 125 with 100 token gradients\n",
            "Processed 12600 token positions (saved 126 batches)\n",
            "💾 Saved optimized batch 126 with 27 token gradients\n",
            "✅ Token-specific collection complete!\n",
            "   Total token gradients: 12627\n",
            "   Saved in 127 batches\n",
            "   Optimized storage: tinystories_gradients/layers\n",
            "✅ Token-specific gradient collection complete!\n",
            "Total token gradients: 12627\n",
            "Total batches saved: 127\n",
            "Optimized storage: tinystories_gradients/layers\n"
          ]
        }
      ],
      "source": [
        "# Collect token-specific gradients (or use existing ones)\n",
        "print(\"🎯 Checking for existing gradients...\")\n",
        "print(\"   Will use existing gradients if found (much faster!)\")\n",
        "print(\"   Otherwise will collect new gradients\")\n",
        "\n",
        "collector = collect_token_gradients(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    stories=stories,\n",
        "    save_dir=SAVE_DIR,\n",
        "    max_samples=MAX_SAMPLES,\n",
        "    max_length=MAX_LENGTH,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    save_batch_size=SAVE_BATCH_SIZE,\n",
        "    token_context_window=TOKEN_CONTEXT_WINDOW,\n",
        "    force_recollect=False  # Set to True if you want to recollect gradients\n",
        ")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 💾 Gradient Collection - Smart Caching\n",
        "\n",
        "The gradient collector now **automatically checks for existing gradients** and reuses them if found:\n",
        "\n",
        "**✅ Found existing gradients**:\n",
        "- Skips collection entirely (saves hours!)\n",
        "- Lists available layers and batch count\n",
        "- Ready to load and visualize immediately\n",
        "\n",
        "**🔄 Need to force recollection**:\n",
        "```python\n",
        "# To recollect gradients (e.g., changed parameters)\n",
        "collector = collect_token_gradients(\n",
        "    # ... other parameters ...\n",
        "    force_recollect=True  # Forces new collection\n",
        ")\n",
        "```\n",
        "\n",
        "**When you might want `force_recollect=True`**:\n",
        "- Changed `MAX_SAMPLES`, `MAX_LENGTH`, or other collection parameters\n",
        "- Want to test with different data\n",
        "- Gradients seem corrupted or incomplete\n",
        "\n",
        "**Current behavior**: Uses existing gradients if found, otherwise collects new ones automatically!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: List all available layers in existing gradients\n",
        "print(\"📋 Listing all available gradient layers...\")\n",
        "list_existing_gradients(SAVE_DIR)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "💾 Loading single layer gradients (memory efficient)...\n",
            "Target layer: transformer.h.0.attn.attention.v_proj.weight\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'collector' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[5], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget layer: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTARGET_LAYER\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Load from optimized storage format\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m gradient_tensors, token_data \u001b[38;5;241m=\u001b[39m load_single_layer_gradients(\u001b[43mcollector\u001b[49m, target_layer\u001b[38;5;241m=\u001b[39mTARGET_LAYER)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gradient_tensors \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m❌ Failed to load layer! Showing available layers...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'collector' is not defined"
          ]
        }
      ],
      "source": [
        "# Load gradients for analysis (optimized single-layer loading)\n",
        "TARGET_LAYER = \"transformer.h.0.attn.attention.v_proj.weight\"  # Specify which layer to analyze\n",
        "'''\n",
        "TARGET_LAYER = \"transformer.h.0.attn.attention.q_proj.weight\"\n",
        "TARGET_LAYER = \"transformer.h.0.attn.attention.k_proj.weight\"\n",
        "TARGET_LAYER = \"transformer.h.0.attn.attention.v_proj.weight\"\n",
        "TARGET_LAYER = \"transformer.h.0.mlp.c_fc.weight\"\n",
        "TARGET_LAYER = \"transformer.h.7.attn.attention.q_proj.weight\"\n",
        "TARGET_LAYER = \"transformer.h.7.mlp.c_fc.weight\"\n",
        "TARGET_LAYER = \"transformer.wte.weight\"\n",
        "TARGET_LAYER = \"transformer.wpe.weight\"\n",
        "'''\n",
        "\n",
        "print(\"💾 Loading single layer gradients (memory efficient)...\")\n",
        "print(f\"Target layer: {TARGET_LAYER}\")\n",
        "\n",
        "# Load from optimized storage format\n",
        "gradient_tensors, token_data = load_single_layer_gradients(collector, target_layer=TARGET_LAYER)\n",
        "\n",
        "if gradient_tensors is None:\n",
        "    print(\"❌ Failed to load layer! Showing available layers...\")\n",
        "    gradient_tensors, token_data = load_single_layer_gradients(collector, target_layer=None)\n",
        "\n",
        "if gradient_tensors is None or token_data is None:\n",
        "    print(\"❌ No data found! Please run gradient collection first.\")\n",
        "else:\n",
        "    print(f\"✅ Data loaded successfully\")\n",
        "    print(f\"Available parameters: {list(gradient_tensors.keys())}\")\n",
        "    print(f\"Total token gradients: {len(token_data)}\")\n",
        "    \n",
        "    # Show memory usage\n",
        "    total_memory = 0\n",
        "    for name, grads in gradient_tensors.items():\n",
        "        mem_mb = grads.nelement() * grads.element_size() / (1024**2)\n",
        "        total_memory += mem_mb\n",
        "        print(f\"  {name}: {grads.shape} ({mem_mb:.1f} MB)\")\n",
        "        \n",
        "    print(f\"📊 Total gradient data in memory: {total_memory:.1f} MB\")\n",
        "    \n",
        "    # Show some example token data\n",
        "    if token_data and len(token_data) > 0:\n",
        "        print(f\"\\n🎯 Example token predictions:\")\n",
        "        for i in range(min(3, len(token_data))):\n",
        "            example = token_data[i]\n",
        "            print(f\"  {i+1}. {example.get('prediction_task', 'Unknown')}\")\n",
        "            print(f\"     Context: {example.get('context_text', 'No context')}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🎨 Creating token-specific t-SNE visualization for: transformer.h.0.attn.attention.v_proj.weight\n",
            "🔍 Computing t-SNE for 10000 token gradients...\n",
            "Gradient shape: torch.Size([10000, 4096])\n",
            "[t-SNE] Computing 91 nearest neighbors...\n",
            "[t-SNE] Indexed 10000 samples in 0.021s...\n",
            "[t-SNE] Computed neighbors for 10000 samples in 2.695s...\n",
            "[t-SNE] Computed conditional probabilities for sample 1000 / 10000\n",
            "[t-SNE] Computed conditional probabilities for sample 2000 / 10000\n",
            "[t-SNE] Computed conditional probabilities for sample 3000 / 10000\n",
            "[t-SNE] Computed conditional probabilities for sample 4000 / 10000\n",
            "[t-SNE] Computed conditional probabilities for sample 5000 / 10000\n",
            "[t-SNE] Computed conditional probabilities for sample 6000 / 10000\n",
            "[t-SNE] Computed conditional probabilities for sample 7000 / 10000\n",
            "[t-SNE] Computed conditional probabilities for sample 8000 / 10000\n",
            "[t-SNE] Computed conditional probabilities for sample 9000 / 10000\n",
            "[t-SNE] Computed conditional probabilities for sample 10000 / 10000\n",
            "[t-SNE] Mean sigma: 0.000000\n",
            "[t-SNE] KL divergence after 250 iterations with early exaggeration: 91.783340\n",
            "[t-SNE] KL divergence after 1000 iterations: 2.936831\n"
          ]
        }
      ],
      "source": [
        "# Create token-specific t-SNE visualization with PCA preprocessing\n",
        "if gradient_tensors is not None and token_data is not None:\n",
        "    print(f\"🎨 Creating token-specific t-SNE visualization for: {TARGET_LAYER}\")\n",
        "    print(f\"💡 Using PCA preprocessing to reduce 4096D → 50D (faster & often better results)\")\n",
        "    \n",
        "    # Create t-SNE visualization with token-specific hover information + PCA\n",
        "    tsne_fig = create_tsne_with_token_data(\n",
        "        gradients=gradient_tensors,\n",
        "        token_data=token_data,\n",
        "        layer_name=TARGET_LAYER,\n",
        "        max_samples=10000,  # Use 10k samples for good visualization\n",
        "        perplexity=30,\n",
        "        show_prediction_task=True,  # Show \"token A → token B\" in hover\n",
        "        use_pca=True,              # NEW: PCA preprocessing (recommended!)\n",
        "        pca_components=50          # Reduce from 4096D to 50D\n",
        "    )\n",
        "    \n",
        "    if tsne_fig:\n",
        "        # Display the figure\n",
        "        tsne_fig.show()\n",
        "        \n",
        "        # Save the visualization\n",
        "        output_file = f\"{SAVE_DIR}/tsne_token_specific_{TARGET_LAYER.replace('.', '_')}.html\"\n",
        "        tsne_fig.write_html(output_file)\n",
        "        print(f\"💾 Visualization saved to: {output_file}\")\n",
        "        print(f\"🔍 Hover over points to see specific token predictions!\")\n",
        "        print(f\"🔬 PCA helps: reduces noise, speeds up t-SNE, often improves clustering\")\n",
        "    \n",
        "else:\n",
        "    print(\"❌ No gradient data available for visualization\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create UMAP visualization with PCA preprocessing (alternative to t-SNE)\n",
        "if gradient_tensors is not None and token_data is not None:\n",
        "    print(\"🗺️ Creating UMAP visualization as alternative to t-SNE...\")\n",
        "    print(\"💡 UMAP often reveals different clustering patterns and preserves global structure better\")\n",
        "    print(\"🔧 Also using PCA preprocessing for consistency and performance\")\n",
        "    \n",
        "    # Create UMAP visualization with token-specific hover information + PCA\n",
        "    # Using more samples to match t-SNE and different parameters for comparison\n",
        "    umap_fig = create_umap_with_token_data(\n",
        "        gradients=gradient_tensors,\n",
        "        token_data=token_data,\n",
        "        layer_name=TARGET_LAYER,\n",
        "        max_samples=2000,  # Use more samples than t-SNE for good coverage\n",
        "        n_neighbors=30,    # Larger neighborhood for global structure\n",
        "        min_dist=0.05,     # Tighter clusters\n",
        "        show_prediction_task=True,\n",
        "        use_pca=True,              # NEW: PCA preprocessing (consistent with t-SNE)\n",
        "        pca_components=50          # Same reduction as t-SNE for comparison\n",
        "    )\n",
        "    \n",
        "    if umap_fig:\n",
        "        # Display the figure\n",
        "        umap_fig.show()\n",
        "        \n",
        "        # Save the visualization\n",
        "        output_file = f\"{SAVE_DIR}/umap_token_specific_{TARGET_LAYER.replace('.', '_')}.html\"\n",
        "        umap_fig.write_html(output_file)\n",
        "        print(f\"💾 UMAP visualization saved to: {output_file}\")\n",
        "        print(f\"🔍 Compare with t-SNE to see different clustering perspectives!\")\n",
        "        print(f\"🎯 UMAP advantages: Better global structure, faster computation, deterministic\")\n",
        "        print(f\"🔬 Both now use PCA 4096D→50D for fair comparison\")\n",
        "    \n",
        "else:\n",
        "    print(\"❌ No gradient data available for UMAP visualization\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 🔬 Why PCA Preprocessing?\n",
        "\n",
        "**Before our update**: We were feeding **4096-dimensional** gradients directly to t-SNE/UMAP\n",
        "- 64×64 weight matrix flattened = 4096 dimensions\n",
        "- Very high-dimensional for embedding algorithms\n",
        "- Slower computation and potentially noisier results\n",
        "\n",
        "**After our update**: We now use **PCA preprocessing** to reduce 4096D → 50D\n",
        "- ✅ **Faster computation**: Much quicker t-SNE/UMAP with 50D vs 4096D\n",
        "- ✅ **Noise reduction**: PCA removes less important variance directions\n",
        "- ✅ **Better clustering**: Often reveals cleaner patterns\n",
        "- ✅ **Retains most information**: 50 components typically capture 80-95% of variance\n",
        "\n",
        "**Performance comparison**:\n",
        "- **Without PCA**: 4096D → ~2-3 minutes for t-SNE\n",
        "- **With PCA**: 50D → ~10-30 seconds for t-SNE (8-10x faster!)\n",
        "\n",
        "The PCA step shows how much variance each component explains, helping you understand if 50 components are sufficient.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
